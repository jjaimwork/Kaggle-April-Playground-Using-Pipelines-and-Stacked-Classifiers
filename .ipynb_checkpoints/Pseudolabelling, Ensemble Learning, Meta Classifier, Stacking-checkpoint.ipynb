{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/remekkinas/ensemble-learning-meta-classifier-for-stacking  \n",
    "https://www.kaggle.com/hiro5299834/tps-apr-2021-pseudo-labeling-voting-ensemble  \n",
    "https://www.kaggle.com/c/tabular-playground-series-apr-2021/discussion/231738  \n",
    "\n",
    "Alright we're diving blind to study and learn this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Description\n",
    "\n",
    "ensemble-learning meta-classifier for stacking using cross-validation\n",
    "\n",
    "rubber duck method\n",
    "> Stacking is an ensemble learning technique where it combines multiple classification models via meta classifier(a meta classifier(level 1) is a proxy, or a standalone representation until it's time for the main(level 2) classifier).  \n",
    "**First level** consists of fitting the training set on different classifiers, which the same training set is also used for the **Second Level** which may lead to overfitting. To avoid that, the algorithm uses k-folds cross validation and are used to fit the **First Level** classifiers,\n",
    "**Different Folds for each First Level Classifier**. They are then stacked as input data for the **Second Level Classifier**. After the training of the **StackingCVClassifier** the first level classifiers are fit to the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load The Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools    \n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score,  KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "submission_df = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "test_df['Survived'] = pd.read_csv('kaggle_submission.csv')['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([train_df,test_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudolabelling  \n",
    "\n",
    ">  the technique of using unlabelled (in some cases test) data with labelled train one to create better models. But how can we use unlabelled data if we need to train supervised model? The answer here is that we can built a 2 stages paradigm for this:  \n",
    "◽ on stage 1 we train model only on train data and predict for unlabelled  \n",
    "◽ on stage 2 we use predictions from stage 1 as «pseudo» labels for unlabelled data, concat with pseudolabelled data with labelled train and fit model on this concated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>3</td>\n",
       "      <td>Holliday, Daniel</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24745</td>\n",
       "      <td>63.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>3</td>\n",
       "      <td>Nguyen, Lorraine</td>\n",
       "      <td>female</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13264</td>\n",
       "      <td>5.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>Harris, Heather</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25990</td>\n",
       "      <td>38.91</td>\n",
       "      <td>B15315</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>2</td>\n",
       "      <td>Larsen, Eric</td>\n",
       "      <td>male</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>314011</td>\n",
       "      <td>12.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>1</td>\n",
       "      <td>Cleary, Sarah</td>\n",
       "      <td>female</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26203</td>\n",
       "      <td>26.89</td>\n",
       "      <td>B22515</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>199995</td>\n",
       "      <td>3</td>\n",
       "      <td>Cash, Cheryle</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7686</td>\n",
       "      <td>10.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>199996</td>\n",
       "      <td>1</td>\n",
       "      <td>Brown, Howard</td>\n",
       "      <td>male</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13004</td>\n",
       "      <td>68.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>199997</td>\n",
       "      <td>3</td>\n",
       "      <td>Lightfoot, Cameron</td>\n",
       "      <td>male</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4383317</td>\n",
       "      <td>10.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>199998</td>\n",
       "      <td>1</td>\n",
       "      <td>Jacobsen, Margaret</td>\n",
       "      <td>female</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>PC 26988</td>\n",
       "      <td>29.68</td>\n",
       "      <td>B20828</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>199999</td>\n",
       "      <td>1</td>\n",
       "      <td>Fishback, Joanna</td>\n",
       "      <td>female</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>PC 41824</td>\n",
       "      <td>195.41</td>\n",
       "      <td>E13345</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId  Pclass                Name     Sex   Age  SibSp  Parch  \\\n",
       "0           100000       3    Holliday, Daniel    male  19.0      0      0   \n",
       "1           100001       3    Nguyen, Lorraine  female  53.0      0      0   \n",
       "2           100002       1     Harris, Heather  female  19.0      0      0   \n",
       "3           100003       2        Larsen, Eric    male  25.0      0      0   \n",
       "4           100004       1       Cleary, Sarah  female  17.0      0      2   \n",
       "...            ...     ...                 ...     ...   ...    ...    ...   \n",
       "99995       199995       3       Cash, Cheryle  female  27.0      0      0   \n",
       "99996       199996       1       Brown, Howard    male  59.0      1      0   \n",
       "99997       199997       3  Lightfoot, Cameron    male  47.0      0      0   \n",
       "99998       199998       1  Jacobsen, Margaret  female  49.0      1      2   \n",
       "99999       199999       1    Fishback, Joanna  female  41.0      0      2   \n",
       "\n",
       "         Ticket    Fare   Cabin Embarked  Survived  \n",
       "0         24745   63.01     NaN        S         0  \n",
       "1         13264    5.81     NaN        S         1  \n",
       "2         25990   38.91  B15315        C         1  \n",
       "3        314011   12.93     NaN        S         0  \n",
       "4         26203   26.89  B22515        C         1  \n",
       "...         ...     ...     ...      ...       ...  \n",
       "99995      7686   10.12     NaN        Q         1  \n",
       "99996     13004   68.31     NaN        S         0  \n",
       "99997   4383317   10.87     NaN        S         0  \n",
       "99998  PC 26988   29.68  B20828        C         1  \n",
       "99999  PC 41824  195.41  E13345        C         1  \n",
       "\n",
       "[100000 rows x 12 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# shuffle the dataset\n",
    "all_df = all_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# dropping the unneeded columns in the dataset\n",
    "unneeded_cols = ['Cabin', 'PassengerId', 'Name', 'Ticket']\n",
    "for col in unneeded_cols:\n",
    "    all_df = all_df.drop([col], axis=1)\n",
    "    train_df = train_df.drop([col], axis=1)\n",
    "    test_df = test_df.drop([col], axis=1)\n",
    "    \n",
    "all_df = all_df[all_df['Embarked'].notna()]\n",
    "all_df = all_df[all_df['Fare'].notna()]\n",
    "#test_df = test_df[test_df['Embarked'].notna()]\n",
    "\n",
    "# converting categorical values to numeric    \n",
    "cat_features = ['Pclass','Sex', 'Embarked', 'SibSp', 'Parch']\n",
    "\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     (\"imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "#     (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "# # filling missing values\n",
    "# num_features = ['Age']\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent'))])\n",
    "\n",
    "# # preprocess and modeling pipeline\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('cat',categorical_transformer, cat_features),\n",
    "#         ('num', numeric_transformer, num_features),\n",
    "#     ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_freq = SimpleImputer(strategy='most_frequent')\n",
    "one_hot = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "for cat in cat_features:\n",
    "    all_df[cat] = all_df[cat].fillna(all_df[cat].value_counts().index[0])\n",
    "    test_df[cat] = test_df[cat].fillna(test_df[cat].value_counts().index[0])\n",
    "\n",
    "all_df['Age'] = all_df['Age'].fillna(all_df['Age'].mode()[0])\n",
    "test_df['Age'] = test_df['Age'].fillna(test_df['Age'].mode()[0])\n",
    "all_df = pd.get_dummies(all_df, columns=cat_features)\n",
    "test_df = pd.get_dummies(test_df, columns=cat_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_df.drop('Survived', axis=1)\n",
    "y = all_df.Survived\n",
    "\n",
    "# split using train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression' : LogisticRegression(solver='liblinear'),\n",
    "    'Ridge Classifier' : RidgeClassifier(),\n",
    "    'SGDClassifier' : SGDClassifier(),\n",
    "    'RandomForest' : RandomForestClassifier(),\n",
    "    'ExtraTreesClassifier' : ExtraTreesClassifier(),\n",
    "    'KNC': KNeighborsClassifier(),\n",
    "    'DecisionTreeClassifier':DecisionTreeClassifier(),\n",
    "    'LGBMClassifier' : LGBMClassifier(verbose=-1),\n",
    "    'XGBClassifier' : XGBClassifier(verbose=0, use_label_encoder=False,),\n",
    "    'CatBoostClassifier' : CatBoostClassifier(verbose=0),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_score(models, X_train, X_test, y_train, y_test):\n",
    "    model_scores = {}\n",
    "    for name, model in models.items(): \n",
    "        model.fit(X_train, y_train)\n",
    "        model_scores[name] = model.score(X_test, y_test)\n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:37:21] WARNING: ..\\src\\learner.cc:573: \n",
      "Parameters: { \"verbose\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:37:21] WARNING: ..\\src\\learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': 0.8534210130013553,\n",
       " 'Ridge Classifier': 0.8339775446346401,\n",
       " 'SGDClassifier': 0.754346334688687,\n",
       " 'RandomForest': 0.8464099861118083,\n",
       " 'ExtraTreesClassifier': 0.8306979234643509,\n",
       " 'KNC': 0.8397168816826465,\n",
       " 'DecisionTreeClassifier': 0.8129444639659991,\n",
       " 'LGBMClassifier': 0.8795408530361595,\n",
       " 'XGBClassifier': 0.8783862925221291,\n",
       " 'CatBoostClassifier': 0.8790388702039723}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores = fit_and_score(models=models, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LGBMClassifier'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "max(model_scores.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop('Survived', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== FOLD 0 =====\n",
      "===== ACCURACY SCORE 0.879594 =====\n",
      "\n",
      "===== FOLD 1 =====\n",
      "===== ACCURACY SCORE 0.886933 =====\n",
      "\n",
      "===== FOLD 2 =====\n",
      "===== ACCURACY SCORE 0.877649 =====\n",
      "\n",
      "===== FOLD 3 =====\n",
      "===== ACCURACY SCORE 0.880572 =====\n",
      "\n",
      "===== FOLD 4 =====\n",
      "===== ACCURACY SCORE 0.881900 =====\n",
      "\n",
      "===== FOLD 5 =====\n",
      "===== ACCURACY SCORE 0.881951 =====\n",
      "\n",
      "===== FOLD 6 =====\n",
      "===== ACCURACY SCORE 0.886760 =====\n",
      "\n",
      "===== FOLD 7 =====\n",
      "===== ACCURACY SCORE 0.877648 =====\n",
      "\n",
      "===== FOLD 8 =====\n",
      "===== ACCURACY SCORE 0.876981 =====\n",
      "\n",
      "===== FOLD 9 =====\n",
      "===== ACCURACY SCORE 0.879043 =====\n",
      "\n",
      "===== ACCURACY SCORE 0.880900 =====\n"
     ]
    }
   ],
   "source": [
    "dtm_oof = np.zeros(train_df.shape[0])\n",
    "dtm_preds = np.zeros(test_df.shape[0])\n",
    "feature_importances = pd.DataFrame()\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(all_df, all_df['Survived'])):\n",
    "    print(f\"===== FOLD {fold} =====\")\n",
    "    oof_idx = np.array([idx for idx in valid_idx if idx < train_df.shape[0]])\n",
    "    preds_idx = np.array([idx for idx in valid_idx if idx >= train_df.shape[0]])\n",
    "\n",
    "    X_train, y_train = all_df.iloc[train_idx].drop('Survived', axis=1), all_df.iloc[train_idx]['Survived']\n",
    "    X_valid, y_valid = all_df.iloc[oof_idx].drop('Survived', axis=1), all_df.iloc[oof_idx]['Survived']\n",
    "    X_test = all_df.iloc[preds_idx].drop('Survived', axis=1)\n",
    "    \n",
    "    model = LGBMClassifier(\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    dtm_oof[oof_idx] = model.predict_proba(X_valid)[:,1]\n",
    "    dtm_preds[preds_idx-train_df.shape[0]] = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    acc_score = accuracy_score(y_valid, np.where(dtm_oof[oof_idx]>0.5, 1, 0))\n",
    "    print(f\"===== ACCURACY SCORE {acc_score:.6f} =====\\n\")\n",
    "    \n",
    "acc_score = accuracy_score(all_df[:train_df.shape[0]]['Survived'], np.where(dtm_oof>0.5, 1, 0))\n",
    "print(f\"===== ACCURACY SCORE {acc_score:.6f} =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop('Survived', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['Survived'] = model.predict(test_df)\n",
    "submission_df.to_csv('LGBMClassifier.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>199995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>199996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>199997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>199998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>199999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId  Survived\n",
       "0           100000         0\n",
       "1           100001         1\n",
       "2           100002         1\n",
       "3           100003         0\n",
       "4           100004         1\n",
       "...            ...       ...\n",
       "99995       199995         1\n",
       "99996       199996         0\n",
       "99997       199997         0\n",
       "99998       199998         1\n",
       "99999       199999         1\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STACKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "submission_df = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "test_df['Survived'] = pd.read_csv('kaggle_submission.csv')['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([train_df,test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Oconnor, Frankie</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>209245</td>\n",
       "      <td>27.14</td>\n",
       "      <td>C12239</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Bryan, Drew</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27323</td>\n",
       "      <td>13.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Owens, Kenneth</td>\n",
       "      <td>male</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA 457703</td>\n",
       "      <td>71.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Kramer, James</td>\n",
       "      <td>male</td>\n",
       "      <td>19.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A. 10866</td>\n",
       "      <td>13.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Bond, Michael</td>\n",
       "      <td>male</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>427635</td>\n",
       "      <td>7.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>199995</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Cash, Cheryle</td>\n",
       "      <td>female</td>\n",
       "      <td>27.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7686</td>\n",
       "      <td>10.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>199996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Brown, Howard</td>\n",
       "      <td>male</td>\n",
       "      <td>59.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13004</td>\n",
       "      <td>68.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>199997</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Lightfoot, Cameron</td>\n",
       "      <td>male</td>\n",
       "      <td>47.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4383317</td>\n",
       "      <td>10.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>199998</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Jacobsen, Margaret</td>\n",
       "      <td>female</td>\n",
       "      <td>49.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>PC 26988</td>\n",
       "      <td>29.68</td>\n",
       "      <td>B20828</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>199999</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Fishback, Joanna</td>\n",
       "      <td>female</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>PC 41824</td>\n",
       "      <td>195.41</td>\n",
       "      <td>E13345</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId  Survived  Pclass                Name     Sex    Age  \\\n",
       "0                0         1       1    Oconnor, Frankie    male    NaN   \n",
       "1                1         0       3         Bryan, Drew    male    NaN   \n",
       "2                2         0       3      Owens, Kenneth    male   0.33   \n",
       "3                3         0       3       Kramer, James    male  19.00   \n",
       "4                4         1       3       Bond, Michael    male  25.00   \n",
       "...            ...       ...     ...                 ...     ...    ...   \n",
       "99995       199995         1       3       Cash, Cheryle  female  27.00   \n",
       "99996       199996         0       1       Brown, Howard    male  59.00   \n",
       "99997       199997         0       3  Lightfoot, Cameron    male  47.00   \n",
       "99998       199998         1       1  Jacobsen, Margaret  female  49.00   \n",
       "99999       199999         1       1    Fishback, Joanna  female  41.00   \n",
       "\n",
       "       SibSp  Parch     Ticket    Fare   Cabin Embarked  \n",
       "0          2      0     209245   27.14  C12239        S  \n",
       "1          0      0      27323   13.35     NaN        S  \n",
       "2          1      2  CA 457703   71.29     NaN        S  \n",
       "3          0      0   A. 10866   13.04     NaN        S  \n",
       "4          0      0     427635    7.76     NaN        S  \n",
       "...      ...    ...        ...     ...     ...      ...  \n",
       "99995      0      0       7686   10.12     NaN        Q  \n",
       "99996      1      0      13004   68.31     NaN        S  \n",
       "99997      0      0    4383317   10.87     NaN        S  \n",
       "99998      1      2   PC 26988   29.68  B20828        C  \n",
       "99999      0      2   PC 41824  195.41  E13345        C  \n",
       "\n",
       "[200000 rows x 12 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "for key, value in model_scores.items():\n",
    "    temp = [key,value]\n",
    "    score_list.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['LGBMClassifier', 0.8795408530361595],\n",
       " ['CatBoostClassifier', 0.8790388702039723],\n",
       " ['XGBClassifier', 0.8783862925221291],\n",
       " ['Logistic Regression', 0.8534210130013553],\n",
       " ['RandomForest', 0.8464099861118083],\n",
       " ['KNC', 0.8397168816826465],\n",
       " ['Ridge Classifier', 0.8339775446346401],\n",
       " ['ExtraTreesClassifier', 0.8306979234643509],\n",
       " ['DecisionTreeClassifier', 0.8129444639659991],\n",
       " ['SGDClassifier', 0.754346334688687]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def takeSecond(score_list):\n",
    "    return score_list[1]\n",
    "score_list.sort(key=takeSecond,reverse=True)\n",
    "score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl1 = LogisticRegression(solver='liblinear')\n",
    "cl2 = RidgeClassifier()\n",
    "cl3 = SGDClassifier()\n",
    "cl4 = RandomForestClassifier()\n",
    "cl5 = ExtraTreesClassifier()\n",
    "cl6 = KNeighborsClassifier()\n",
    "cl7 = DecisionTreeClassifier()\n",
    "cl8 = LGBMClassifier(device = 'gpu',gpu_platform_id = '1', gpu_device_id = '1')\n",
    "cl9 = XGBClassifier(eval_metric='binary:logistic' )\n",
    "cl10 = CatBoostClassifier(verbose = None, logging_level = 'Silent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"LGBMClassifier\": cl8,\n",
    "    \"XGBClassifier\": cl9,\n",
    "    \"CatBoostClassifier\": cl10,\n",
    "    \"RandomForest\": cl4,\n",
    "    \"LogisticRegression\": cl1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "taken_classifiers = ['LGBMClassifier', 'XGBClassifier', 'CatBoostClassifier', 'RandomForest', 'LogisticRegression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr=LogisticRegression()\n",
    "\n",
    "def best_stacking_search():\n",
    "    cls_list = []\n",
    "    best_auc = -1\n",
    "    i=0\n",
    "\n",
    "    best_cls_experiment = list()\n",
    "\n",
    "    print(\">>>> Training started <<<<\")\n",
    "\n",
    "    for cls_comb in range(2, len(taken_classifiers)+1):\n",
    "        for subset in itertools.combinations(taken_classifiers, cls_comb):\n",
    "            cls_list.append(subset)\n",
    "\n",
    "    print(f\"Total number of model combination: {len(cls_list)}\")\n",
    "\n",
    "\n",
    "    for cls_exp in cls_list:\n",
    "        cls_labels = list(cls_exp)\n",
    "\n",
    "        classifier_exp = []\n",
    "        for ii in range(len(cls_labels)):\n",
    "            label = taken_classifiers[ii]\n",
    "            classifier = classifiers[label]\n",
    "            classifier_exp.append(classifier)\n",
    "\n",
    "\n",
    "        sclf = StackingCVClassifier(classifiers = classifier_exp,\n",
    "                                    shuffle = False,\n",
    "                                    use_probas = True,\n",
    "                                    cv = 5,\n",
    "                                    meta_classifier = mlr,\n",
    "                                    n_jobs = -1)\n",
    "        model = sclf\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        scores = model.score(X_test, y_test)\n",
    "\n",
    "        if scores.mean() > best_auc:\n",
    "            best_cls_experiment = list(cls_exp)\n",
    "        i += 1\n",
    "        print(f\"  {i} - Stacked combination - Acc {cls_exp}: {scores.mean():.5f}\")\n",
    "        \n",
    "    return best_cls_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Training started <<<<\n",
      "Total number of model combination: 26\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-4a31316b2f5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_stacking_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-75-a50c21fe0c51>\u001b[0m in \u001b[0;36mbest_stacking_search\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msclf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mlxtend\\classifier\\stacking_cv_classification.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_name_estimators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m             prediction = cross_val_predict(\n\u001b[0m\u001b[0;32m    250\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfinal_cv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[0;32m    769\u001b[0m     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n\u001b[0;32m    770\u001b[0m                         pre_dispatch=pre_dispatch)\n\u001b[1;32m--> 771\u001b[1;33m     prediction_blocks = parallel(delayed(_fit_and_predict)(\n\u001b[0m\u001b[0;32m    772\u001b[0m         clone(estimator), X, y, train, test, verbose, fit_params, method)\n\u001b[0;32m    773\u001b[0m         for train, test in cv.split(X, y, groups))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1062\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    938\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLightGBMError\u001b[0m: GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1"
     ]
    }
   ],
   "source": [
    "best_stacking_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
